{"cells":[{"cell_type":"markdown","source":["## Fabric Databricks Mirroring notebook\n","\n","Purpose: enable automation and finer control of creation of Fabric Databrick Mirror item, improving readability and audit\n","\n","- Use UC APIs to collect appropriate metadata on catalog, schema, table and tag metadata - Note - specific schema names as input\n","- Assumption is that there are tags called Domain and Subdomain available in UC. If there is no domain tag, then \"undefined domain\" will be the name of the Databricks Mirror item\n","- Use Fabric APIs to create appropriate Databricks mirror items"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0db16485-a352-41c6-9346-a7d6cfab4460"},{"cell_type":"markdown","source":["#### Prereqs\n","\n","- review https://learn.microsoft.com/en-us/fabric/mirroring/azure-databricks-tutorial\n","- ensure USE EXTERNAL SCHEMA at catalog or schema level in UC as appropriate to relevant users\n","- create appropriate connection in Fabric to connect to UC - use a naming convention to ease reuse\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bcc6f52a-d4af-497d-a2c4-ccf7d548a028"},{"cell_type":"code","source":["%pip install semantic-link-labs"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"jupyter":{"outputs_hidden":true}},"id":"50933df9-39f1-4abb-a234-bde8650952b3"},{"cell_type":"code","source":["import base64\n","import json\n","import requests\n","from collections import defaultdict\n","from notebookutils import mssparkutils\n","import sempy.fabric as fabric  \n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"984c000e-707f-4fe5-945f-a83ea5b25d4e"},{"cell_type":"code","source":["fabric_ws_akv = \"https://fab279akv1.vault.azure.net/\"\n","adb_ws_id_s = \"databricks-ws-id\"\n","adb_ws_token_s= \"databricks-ws-token\"\n","\n","# get the Databricks workspace id and PAT\n","dbx_workspace = notebookutils.credentials.getSecret(\n","    fabric_ws_akv,\n","    adb_ws_id_s)\n","\n","dbx_token = notebookutils.credentials.getSecret(\n","    fabric_ws_akv,\n","    adb_ws_token_s)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"jupyter":{"source_hidden":false}},"id":"d225aa40-6ad6-4af7-a146-946b23cc5eba"},{"cell_type":"code","source":["# Databricks workspace\n","\n","# Unity Catalog\n","dbx_uc_catalog = \"databricks279wus3\"\n","#dbx_uc_schemas = '[\"bakers\", \"publicchange2\", \"nyctaxi\"]'\n","dbx_uc_schemas = '[\"healthv\"]'\n","#if using the current workspace leave this, otherwise replace it with target workspace id\n","workspace_id = fabric.get_workspace_id()  \n","print(f' Workspace id is : {workspace_id}') \n","# Fabric \n","fab_workspace_id = workspace_id\n","fab_adls_connection_id = \"\"\n","fab_dbx_workspace_connection_id = \"469a508f-77d4-4a5f-a629-251fd0d3b4bf\t\""],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5813c6ee-00d7-46eb-b074-afc1d29ef806"},{"cell_type":"markdown","source":["#### Databricks Utilities\n","(explanation of code inside Utils class below)\n","###### a. get_dbx_uc_schemas(databricks_config)\n","- Purpose: Fetches schemas from a specified Databricks Unity Catalog, and for each schema, retrieves its associated tags.\n","- How it works:Reads Databricks workspace URL, token, catalog, and list of schemas from databricks_config.\n","Calls the Unity Catalog API to list schemas in the specified catalog.\n","Filters schemas to only those listed in dbx_uc_schemas.\n","For each schema, fetches its tags via another API call.\n","Returns a list of schema info dictionaries, each possibly containing a tags field.\n","###### b. _normalize_tag_map(tag_list)\n","- Purpose:Converts a list of tag dictionaries into a simple {key: value} mapping.\n","- How it works:Iterates over the tag list, extracting the key and value from each tag, handling different possible field names.\n","###### c. group_schemas_by_domain(schemas_with_tags)\n","- Purpose: Groups schemas by their domain and subdomain tags.\n","- How it works: For each schema, normalizes its tags.\n","Extracts domain and subdomain (defaults to \"undefined_domain\"/\"undefined_subdomain\" if missing).\n","Groups full schema names under their domain/subdomain.\n","Returns a nested dictionary: {domain: {subdomain: [full_schema_names]}}.\n","##### Fabric Utilities\n","###### a. _group_fullnames_by_catalog(fullnames)\n","- Purpose:Groups a list of catalog.schema strings by catalog.\n","- How it works:Splits each string at the first dot, grouping schemas under their catalog.\n","###### b. _get_fabric_token(audience)\n","- Purpose:Obtains an access token for Microsoft Fabric APIs.\n","- How it works:Uses notebookutils.credentials.getToken to get a token for the specified audience.\n","###### c. create_uc_mirroring(fabric_config, grouped_by_domain, auto_sync=True, create_if_missing=True)\n","- Purpose:For each group of schemas (by domain/subdomain), creates a mirrored Azure Databricks catalog in Microsoft Fabric via API.\n","- How it works:Reads Fabric workspace and connection IDs from fabric_config.\n","Gets an access token.\n","For each domain/subdomain group, groups schemas by catalog.\n","For each catalog, builds a payload describing the schemas to mirror.\n","Encodes the payload in base64 and wraps it in a higher-level definition.\n","Sends a POST request to the Fabric API to create the mirrored catalog.\n","Collects and returns the results of successful creations.\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c1f67494-20fa-4996-b42e-89bf1d7c9a2e"},{"cell_type":"markdown","source":["### Class and Functions"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7281ec76-3d3c-40ca-afb4-50ce785e437a"},{"cell_type":"code","source":["class Utils:\n","    FABRIC_API_ENDPOINT = \"api.fabric.microsoft.com\"\n","    ONELAKE_API_ENDPOINT = \"onelake.dfs.fabric.microsoft.com\"\n","\n","    # ---------------------------\n","    # DBX utils\n","    # ---------------------------\n","\n","    @staticmethod\n","    def get_dbx_uc_schemas(databricks_config):\n","        all_schemas = []\n","\n","        dbx_workspace = databricks_config[\"dbx_workspace\"].rstrip(\"/\")\n","        dbx_token = databricks_config[\"dbx_token\"]\n","        dbx_uc_catalog = databricks_config[\"dbx_uc_catalog\"]\n","        dbx_uc_schemas = databricks_config[\"dbx_uc_schemas\"]\n","\n","        url = f\"{dbx_workspace}/api/2.1/unity-catalog/schemas?catalog_name={dbx_uc_catalog}\"\n","        headers = {\n","            \"Authorization\": f\"Bearer {dbx_token}\",\n","            \"Content-Type\": \"application/json\",\n","        }\n","        response = requests.get(url, headers=headers)\n","\n","        if response.status_code != 200:\n","            print(f\"! Upps [{response.status_code}] Cannot connect to Unity Catalog. Please review configs.\")\n","            return None\n","\n","        schemas = response.json().get(\"schemas\", [])\n","        schemas_to_process = [s for s in schemas if s[\"name\"] in dbx_uc_schemas]\n","\n","        for schema_info in schemas_to_process:\n","            schema_name = schema_info[\"name\"]\n","            full_schema_name = f\"{dbx_uc_catalog}.{schema_name}\"\n","\n","            tags_url = f\"{dbx_workspace}/api/2.1/unity-catalog/entity-tag-assignments/schemas/{full_schema_name}/tags\"\n","            tag_response = requests.get(tags_url, headers=headers)\n","\n","            if tag_response.status_code == 200:\n","                schema_info[\"tags\"] = tag_response.json().get(\"tag_assignments\", [])\n","            else:\n","                schema_info[\"tags\"] = []\n","                print(f\"! Failed to fetch tags for {full_schema_name}: {tag_response.status_code}\")\n","\n","            all_schemas.append(schema_info)\n","\n","        return all_schemas\n","\n","    @staticmethod\n","    def _normalize_tag_map(tag_list):\n","        tag_map = {}\n","        for t in tag_list or []:\n","            k = t.get(\"key\") or t.get(\"tag_key\") or t.get(\"name\")\n","            v = t.get(\"value\") or t.get(\"tag_value\")\n","            if k is not None:\n","                tag_map[str(k)] = v\n","        return tag_map\n","\n","    @staticmethod\n","    def group_schemas_by_domain(schemas_with_tags):\n","        grouped = defaultdict(lambda: defaultdict(list))\n","\n","        for s in schemas_with_tags:\n","            catalog = s.get(\"catalog_name\", \"\")\n","            schema_name = s.get(\"name\", \"\")\n","            full_name = f\"{catalog}.{schema_name}\" if catalog and schema_name else s.get(\"entity_name\", schema_name)\n","\n","            tag_map = Utils._normalize_tag_map(s.get(\"tags\", []))\n","            domain = tag_map.get(\"domain\", \"undefined_domain\")\n","            # subdomain = tag_map.get(\"subdomain\", \"undefined_subdomain\")\n","            subdomain = tag_map.get(\"subdomain\", \"\")\n","\n","            grouped[domain][subdomain].append(full_name)\n","\n","        return {d: dict(sub) for d, sub in grouped.items()}\n","\n","    # ---------------------------\n","    # Fabric utils\n","    # ---------------------------\n","\n","    @staticmethod\n","    def _group_fullnames_by_catalog(fullnames):\n","        by_cat = defaultdict(list)\n","        for full in fullnames:\n","            if not full or \".\" not in full:\n","                raise ValueError(f\"Bad schema fullname: {full!r} (expected 'catalog.schema')\")\n","            cat, sch = full.split(\".\", 1)\n","            by_cat[cat].append(sch)\n","        return {c: sorted(set(schs)) for c, schs in by_cat.items()}\n","\n","    @staticmethod\n","    def _get_fabric_token(audience=\"https://analysis.windows.net/powerbi/api\"):\n","        from notebookutils import credentials\n","        return credentials.getToken(audience)\n","\n","    @staticmethod\n","    def create_uc_mirroring(fabric_config, grouped_by_domain, auto_sync=True, create_if_missing=True):\n","        workspace_id = fabric_config[\"workspace_id\"]\n","        adls_connection_id = fabric_config[\"adls_connection_id\"]  # kept (even if unused)\n","        dbx_workspace_connection_id = fabric_config[\"dbx_workspace_connection_id\"]\n","\n","        access_token = fabric_config.get(\"access_token\") or Utils._get_fabric_token()\n","        base_uri = f\"https://api.fabric.microsoft.com/v1/workspaces/{workspace_id}/mirroredAzureDatabricksCatalogs\"\n","\n","        headers = {\n","            \"Authorization\": f\"Bearer {access_token}\",\n","            \"Content-Type\": \"application/json\",\n","        }\n","\n","        results = []\n","        for domain, sub_map in (grouped_by_domain or {}).items():\n","            for subdomain, fullnames in (sub_map or {}).items():\n","                by_catalog = Utils._group_fullnames_by_catalog(fullnames)\n","\n","                for catalog_name, schema_names in by_catalog.items():\n","                    #display_name = f\"{domain}_{subdomain}\"\n","                    display_name = f\"{domain}{subdomain}\"\n","\n","                    payload = {\n","                        \"$schema\": \"https://developer.microsoft.com/json-schemas/fabric/item/mirroredAzureDatabricksCatalog/definition/mirroredAzureDatabricksCatalogDefinition/1.0.0/schema.json\",\n","                        \"catalogName\": catalog_name,\n","                        \"databricksWorkspaceConnectionId\": dbx_workspace_connection_id,\n","                        \"mirroringMode\": \"Partial\",\n","                        \"autoSync\": \"Disabled\",\n","                        \"mirrorConfiguration\": {\n","                            \"schemas\": [{\"name\": s, \"mirroringMode\": \"Full\"} for s in schema_names]\n","                        },\n","                    }\n","\n","                    json_payload = json.dumps(payload)\n","                    encoded_content = base64.b64encode(json_payload.encode(\"utf-8\")).decode(\"utf-8\")\n","\n","                    payload_dict = {\n","                        \"displayName\": f\"{display_name}\",\n","                        \"description\": \"Auto-created\",\n","                        \"definition\": {\n","                            \"parts\": [\n","                                {\n","                                    \"path\": \"definition.json\",\n","                                    \"payload\": encoded_content,\n","                                    \"payloadType\": \"InlineBase64\",\n","                                }\n","                            ]\n","                        },\n","                    }\n","\n","                    json_payload = json.dumps(payload_dict)\n","\n","                    resp = requests.post(base_uri, headers=headers, data=json_payload)\n","                    # handle responses              \n","                    respnse_json = resp.json()\n","\n","                    if resp.status_code in (200, 201, 202):\n","                        results.append(resp.json())\n","                        print(f\"∟ Item created for {display_name}. Catalog: {catalog_name} | Schemas: {schema_names}\")\n","                    else:\n","                        error = respnse_json.get('errorCode')\n","                        message = respnse_json.get('message')\n","                        print(f\"∟ Something went wrong in creating the mirrored items for item {display_name}\\n  Catalog: {catalog_name} | Schemas: {schema_names}, response: {resp.status_code} with errorCode: {error} and message: {message} \\n  -- check prereqs -- \")\n","\n","        return results"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c7da9308-ad29-45fd-8985-d95d40e09f93"},{"cell_type":"markdown","source":["#### Check UC and create Databricks mirrored item code"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"2e7b1733-a89f-4178-b21e-2c3bc495eed1"},{"cell_type":"code","source":["def sync_dbx_uc_schemas_to_fabric(databricks_config, fabric_config):\n","    print(\"Checking Unity Catalog schemas and tags...\")\n","    schemas = Utils.get_dbx_uc_schemas(databricks_config)\n","    schemas_by_domain_subdomain = Utils.group_schemas_by_domain(schemas)\n","    print(f\"∟ Tags/schemas found: {schemas_by_domain_subdomain}\")\n","    print(\"Creating Unity Catalog mirroring items...\")\n","    Utils.create_uc_mirroring(fabric_config, schemas_by_domain_subdomain)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"504e16cb-94b5-4382-aa28-2c50deb9615e"},{"cell_type":"markdown","source":["#### Check UC and create Databricks mirrored item with config"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"63f6f3de-28fd-49d6-ad6f-0750e7d90c1c"},{"cell_type":"code","source":["databricks_config = {\n","    'dbx_workspace': dbx_workspace,\n","    'dbx_token': dbx_token,\n","    'dbx_uc_catalog': dbx_uc_catalog,\n","    'dbx_uc_schemas': json.loads(dbx_uc_schemas)\n","}\n","\n","fabric_config = {\n","    'workspace_id': fab_workspace_id,\n","    'adls_connection_id': fab_adls_connection_id,\n","    \"dbx_workspace_connection_id\": fab_dbx_workspace_connection_id\n","}\n","\n","sync_dbx_uc_schemas_to_fabric(databricks_config, fabric_config)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fc094be3-b35b-4544-801a-6b34afad1aee"},{"cell_type":"code","source":[],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"679395f9-1538-43d9-9aff-4e6457174fd9"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{}},"nbformat":4,"nbformat_minor":5}